{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "from utils import plot_mats\n",
    "\n",
    "file_name = 'mnist_test_seq.npy'\n",
    "url = 'http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy'\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    print(\"could not find dataset: download it..\")\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(\"download complete\")\n",
    "\n",
    "# Moving Mnist: 10.000 sequences of length 20 showing 2 digits moving in 64x64\n",
    "moving_mnist = np.load(file_name) # shape: 20,10000,64,64\n",
    "moving_mnist = np.rollaxis(moving_mnist, 1) # --> 10000,20,64,64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ImageSplitter:\n",
    "    \"\"\" splits the video set into shorts of n frames\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 dataset,\n",
    "                 n=3, \n",
    "                 ntest=256,\n",
    "                 batchsize=64):\n",
    "        \"\"\" ctor\n",
    "            dataset: list of videos that have to be cut into n-grams\n",
    "            n: integer, length of subsequence\n",
    "            ntest: integer, number of items that belong to the test set\n",
    "            batchsize: ~\n",
    "            \n",
    "        The data has to be arranged in following order:\n",
    "            NUMBER_OF_VIDEOS, LENGTH_OF_VIDEO, HEIGHT, WIDTH\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.batchsize = batchsize\n",
    "        self.batch_loop = 0\n",
    "        self.n = n\n",
    "        \n",
    "\n",
    "        np.random.shuffle(dataset)\n",
    "        \n",
    "        self.test_set = dataset[0:ntest]\n",
    "        self.train_set = dataset[ntest:]\n",
    "        \n",
    "        F, N, H, W = dataset.shape # Frames, Numbers, Height, Width\n",
    "        \n",
    "        \n",
    "        self.vector_dimension = H * W\n",
    "        self.same_batch_run = True\n",
    "        \n",
    "    def get_dimension(self):\n",
    "        \"\"\" gets the data dimension\n",
    "        \"\"\"\n",
    "        return self.vector_dimension\n",
    "\n",
    "    def transform_to_n_gram(self, batch, ngram=None):\n",
    "        \"\"\" transforms the batch into an n-gram (parameter n)\n",
    "        batch: np.array((batchsize, 20, 64, 64))\n",
    "        \"\"\"\n",
    "        if ngram is None:\n",
    "            n = self.n\n",
    "        else:\n",
    "            n = ngram\n",
    "        N, M, H, W = batch.shape\n",
    "        seqs = M-n+1 # sequences per video\n",
    "        \n",
    "        Result = np.zeros((seqs * N, n, H, W))\n",
    "        \n",
    "        pos = 0\n",
    "        for j in range(N):\n",
    "            for i in range(seqs):\n",
    "                Result[pos] = batch[j, i:i+n]\n",
    "                pos += 1\n",
    "        \n",
    "        N, M, H, W = Result.shape\n",
    "        return Result.reshape((N, M, H * W))\n",
    "    \n",
    "    def is_same_batch_run(self):\n",
    "        if self.same_batch_run:\n",
    "            return True\n",
    "        else:\n",
    "            self.same_batch_run = True\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def next_batch(self, ngram=None):\n",
    "        \"\"\" returns the next batch\n",
    "        \"\"\"\n",
    "        start = self.batch_loop\n",
    "        end = self.batch_loop + self.batchsize\n",
    "        N = self.train_set.shape[0]\n",
    "        if N > end:\n",
    "            self.same_batch_run = True\n",
    "            self.batch_loop = end\n",
    "            return self.transform_to_n_gram(\n",
    "                self.train_set[start:end], ngram=ngram)\n",
    "        else:\n",
    "            self.same_batch_run = False\n",
    "            diff = (N - start)\n",
    "            end = self.batchsize - diff\n",
    "            \n",
    "            set1 = self.train_set[start:N]\n",
    "            set2 = self.train_set[0:end]\n",
    "            \n",
    "            self.batch_loop = end\n",
    "            return self.transform_to_n_gram(\n",
    "                np.concatenate((set1, set2)), ngram=ngram)\n",
    "    \n",
    "    def get_test(self, ngram=None):\n",
    "        \"\"\" returns the test data as ngram\n",
    "        \"\"\"\n",
    "        return self.transform_to_n_gram(self.test_set, ngram=ngram)\n",
    "\n",
    "# splitter = ImageSplitter(moving_mnist, n=3)\n",
    "# batch = splitter.next_batch(ngram=2)\n",
    "# print(batch[:,0,:].shape)\n",
    "\n",
    "# test = splitter.get_test(ngram=2)\n",
    "# print(test.shape)\n",
    "\n",
    "# print(np.max(test), np.min(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx\n",
      "Pretrain: Epoch: 000/100 cost: 2496.105263158\n",
      "Pretrain: Epoch: 001/100 cost: 2496.105674342\n",
      "Pretrain: Epoch: 002/100 cost: 2496.104851974\n",
      "Pretrain: Epoch: 003/100 cost: 2496.105263158\n",
      "Pretrain: Epoch: 004/100 cost: 2496.105468750\n",
      "Pretrain: Epoch: 005/100 cost: 2496.104029605\n",
      "Pretrain: Epoch: 006/100 cost: 2496.104440789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-09417d166611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictiveGatingPyramid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-09417d166611>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, epochs, pre_epochs, print_debug)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_same_batch_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0d3b6520cc80>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self, ngram)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             return self.transform_to_n_gram(\n\u001b[0;32m---> 81\u001b[0;31m                 self.train_set[start:end], ngram=ngram)\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msame_batch_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0d3b6520cc80>\u001b[0m in \u001b[0;36mtransform_to_n_gram\u001b[0;34m(self, batch, ngram)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# this function is 'borrowed' from keras\n",
    "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n",
    "    \"\"\"Returns a tensor with random binomial distribution of values.\n",
    "    # Arguments\n",
    "        shape: A tuple of integers, the shape of tensor to create.\n",
    "        p: A float, `0. <= p <= 1`, probability of binomial distribution.\n",
    "        dtype: String, dtype of returned tensor.\n",
    "        seed: Integer, random seed.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    if dtype is None:\n",
    "        dtype = 'float32'\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(10e6)\n",
    "    return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n",
    "                    tf.ones(shape, dtype=dtype),\n",
    "                    tf.zeros(shape, dtype=dtype))\n",
    "\n",
    "\n",
    "class PredictiveGatingPyramid:\n",
    "    \"\"\" implementation of the pgp\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 depth=2, \n",
    "                 numfilters=512, \n",
    "                 numHidden=256,\n",
    "                 learning_rate=0.01,\n",
    "                 normalize_data=True):\n",
    "        assert depth > 0\n",
    "        assert depth == 2, \"Other depth than 2 not supported\"\n",
    "        self.depth = depth\n",
    "        self.F = numfilters\n",
    "        self.H = numHidden\n",
    "        self.learningRate = learning_rate\n",
    "        self.is_trained = False\n",
    "        self.normalize_data = normalize_data\n",
    "    \n",
    "    \n",
    "    def train(self, X, epochs=100, pre_epochs=100, print_debug=True):\n",
    "        \"\"\" trains the model\n",
    "        \n",
    "            X: training data: must be organized as follows:\n",
    "                Number_of_videos, video_length, H, W\n",
    "            epochs: number of epochs to run for final training\n",
    "            pre_epochs: number of epochs for training initial layer\n",
    "        \"\"\"\n",
    "      \n",
    "        X = X.astype('float32')  # hopefully we don't run OOMem here..\n",
    "        \n",
    "        if self.normalize_data:\n",
    "            X -= X.mean(0)[None, :]\n",
    "            X /= X.std(0)[None, :] + X.std() * 0.1\n",
    "\n",
    "        splitter = ImageSplitter(X, n=self.depth+1)\n",
    "\n",
    "        F = self.F\n",
    "        H = self.H\n",
    "        lr = self.learningRate\n",
    "        dim = splitter.get_dimension()\n",
    "        numpy_rng = np.random.RandomState(1)\n",
    "        \n",
    "        # pretrain the early layer for faster convergence\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, [None, dim])\n",
    "        y = tf.placeholder(tf.float32, [None, dim])\n",
    "        \n",
    "        U1 = tf.Variable(tf.random_normal(shape=(dim, F)) * 0.01)\n",
    "        V1 = tf.Variable(tf.random_normal(shape=(dim, F)) * 0.01)\n",
    "        W1 = tf.Variable(\n",
    "            numpy_rng.uniform(low=-0.01, high=+0.01, \n",
    "                              size=( F, H)).astype('float32'))\n",
    "\n",
    "        b_U1 = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "        b_V1 = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "        b_W1 = tf.Variable(np.zeros(H, dtype='float32'))\n",
    "        b_U1_out = tf.Variable(np.zeros(dim, dtype='float32'))\n",
    "        b_V1_out = tf.Variable(np.zeros(dim, dtype='float32'))\n",
    "        b_W1_out = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "        \n",
    "        \n",
    "        m1 = tf.sigmoid(tf.matmul(tf.multiply(\n",
    "            tf.matmul(x,U1) + b_U1,\n",
    "            tf.matmul(y,V1) + b_V1), W1) + b_W1)\n",
    "\n",
    "        ox = tf.matmul(tf.multiply(\n",
    "                tf.matmul(m1,tf.transpose(W1)) + b_W1_out,\n",
    "                tf.matmul(y,V1) + b_V1),tf.transpose(U1))+ b_U1_out\n",
    "        oy = tf.matmul(tf.multiply(\n",
    "                tf.matmul(m1,tf.transpose(W1)) + b_W1_out,\n",
    "                tf.matmul(x,U1) + b_U1), \n",
    "            tf.transpose(V1)) + b_V1_out\n",
    "\n",
    "        cost_1 = tf.nn.l2_loss(ox-x) + tf.nn.l2_loss(oy-y)\n",
    "        optimizer_1 = tf.train.AdamOptimizer(learning_rate=0.0001)\\\n",
    "            .minimize(cost_1)\n",
    "            \n",
    "        norm_U1 = tf.nn.l2_normalize(U1, [0,1], epsilon=1e-12, name=None)\n",
    "        norm_V1 = tf.nn.l2_normalize(V1, [0,1], epsilon=1e-12, name=None)\n",
    "\n",
    "        normalize_U1 = U1.assign(norm_U1)\n",
    "        normalize_V1 = V1.assign(norm_V1)\n",
    "        \n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            \n",
    "            test_set = splitter.get_test(ngram=2)\n",
    "            X_ = test_set[:,0,:]\n",
    "            Y_ = test_set[:,1,:]\n",
    "            n = test_set.shape[0]\n",
    "            for epoch in range(pre_epochs):\n",
    "                while splitter.is_same_batch_run():\n",
    "                    batch = splitter.next_batch(ngram=2)\n",
    "                    \n",
    "                \n",
    "                cost_ = sess.run(cost_1, feed_dict={x: X_, y: Y_}) / n\n",
    "                if print_debug:\n",
    "                    print (\"Pretrain: Epoch: %03d/%03d cost: %.9f\" %\\\n",
    "                               (epoch,pre_epochs ,cost_) )\n",
    "            \n",
    "        \n",
    "        print(\"train end\")\n",
    "\n",
    "print('xx')\n",
    "\n",
    "model = PredictiveGatingPyramid(depth=2)\n",
    "model.train(moving_mnist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
