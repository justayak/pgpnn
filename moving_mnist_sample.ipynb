{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "from utils import plot_mats\n",
    "from pgpnn import ImageSplitter\n",
    "\n",
    "file_name = 'mnist_test_seq.npy'\n",
    "url = 'http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy'\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    print(\"could not find dataset: download it..\")\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(\"download complete\")\n",
    "\n",
    "# Moving Mnist: 10.000 sequences of length 20 showing 2 digits moving in 64x64\n",
    "moving_mnist = np.load(file_name) # shape: 20,10000,64,64\n",
    "moving_mnist = np.rollaxis(moving_mnist, 1) # --> 10000,20,64,64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx\n",
      "pre-training ended\n",
      "Training: Epoch: 000/100 cost: 882.934678819\n",
      "Training: Epoch: 001/100 cost: 882.929199219\n",
      "Training: Epoch: 002/100 cost: 882.921820747\n",
      "Training: Epoch: 003/100 cost: 882.916124132\n",
      "Training: Epoch: 004/100 cost: 882.910481771\n",
      "Training: Epoch: 005/100 cost: 882.905490451\n",
      "Training: Epoch: 006/100 cost: 882.899359809\n",
      "Training: Epoch: 007/100 cost: 882.894368490\n",
      "Training: Epoch: 008/100 cost: 882.889214410\n",
      "Training: Epoch: 009/100 cost: 882.883843316\n",
      "Training: Epoch: 010/100 cost: 882.879014757\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff7c51f9cf01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0msave_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mload_first_stage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     force_pretrain_first_stage=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ff7c51f9cf01>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, epochs, pre_epochs, print_debug, load_first_stage, force_pretrain_first_stage, learningRate, save_results)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_same_batch_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                     \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                     \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pgpnn/pgpnn.py\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self, ngram)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             return self.transform_to_n_gram(\n\u001b[0;32m---> 83\u001b[0;31m                 self.train_set[start:end], ngram=ngram)\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msame_batch_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pgpnn/pgpnn.py\u001b[0m in \u001b[0;36mtransform_to_n_gram\u001b[0;34m(self, batch, ngram)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# this function is 'borrowed' from keras\n",
    "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n",
    "    \"\"\"Returns a tensor with random binomial distribution of values.\n",
    "    # Arguments\n",
    "        shape: A tuple of integers, the shape of tensor to create.\n",
    "        p: A float, `0. <= p <= 1`, probability of binomial distribution.\n",
    "        dtype: String, dtype of returned tensor.\n",
    "        seed: Integer, random seed.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    if dtype is None:\n",
    "        dtype = 'float32'\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(10e6)\n",
    "    return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n",
    "                    tf.ones(shape, dtype=dtype),\n",
    "                    tf.zeros(shape, dtype=dtype))\n",
    "\n",
    "\n",
    "class PredictiveGatingPyramid:\n",
    "    \"\"\" implementation of the pgp\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 depth=2, \n",
    "                 numfilters=512, \n",
    "                 numHidden=256,\n",
    "                 modelname=None,\n",
    "                 normalize_data=True):\n",
    "        assert depth > 0\n",
    "        assert depth == 2, \"Other depth than 2 not supported\"\n",
    "        self.depth = depth\n",
    "        self.F = numfilters\n",
    "        self.H = numHidden\n",
    "        self.is_trained = False\n",
    "        self.normalize_data = normalize_data\n",
    "        self.modelname = modelname\n",
    "        self.is_first_layer_trained = False\n",
    "        \n",
    "        \n",
    "    def save_first_stage(self):\n",
    "        \"\"\" saves the model onto disk\n",
    "        \"\"\"\n",
    "        modelname = self.modelname\n",
    "        assert(self.is_trained)\n",
    "        assert modelname is not None\n",
    "        np.save(modelname + \"U1\", self.U1_np)\n",
    "        np.save(modelname + \"V1\", self.V1_np)\n",
    "        np.save(modelname + \"W1\", self.W1_np)\n",
    "        np.save(modelname + \"b_U1\", self.b_U1_np)\n",
    "        np.save(modelname + \"b_V1\", self.b_V1_np)\n",
    "        np.save(modelname + \"b_W1\", self.b_W1_np)\n",
    "        np.save(modelname + \"b_U1_out\", self.b_U1_out_np)\n",
    "        np.save(modelname + \"b_V1_out\", self.b_V1_out_np)\n",
    "        np.save(modelname + \"b_W1_out\", self.b_W1_out_np)\n",
    "    \n",
    "    def load_first_stage(self, modelname=None):\n",
    "        \"\"\" loads the first layer of the network\n",
    "        \"\"\"\n",
    "        if modelname is None:\n",
    "            modelname = self.modelname\n",
    "        self.U1_np = np.load(modelname + \"U1.npy\")\n",
    "        self.V1_np = np.load(modelname + \"V1.npy\")\n",
    "        self.W1_np = np.load(modelname + \"W1.npy\")\n",
    "        self.b_U1_np = np.load(modelname + \"b_U1.npy\")\n",
    "        self.b_V1_np = np.load(modelname + \"b_V1.npy\")\n",
    "        self.b_W1_np = np.load(modelname + \"b_W1.npy\")\n",
    "        self.b_U1_out_np = np.load(modelname + \"b_U1_out.npy\")\n",
    "        self.b_V1_out_np = np.load(modelname + \"b_V1_out.npy\")\n",
    "        self.b_W1_out_np = np.load(modelname + \"b_W1_out.npy\")\n",
    "        self.is_first_layer_trained = True\n",
    "    \n",
    "    \n",
    "    def train(self, X, epochs=100, pre_epochs=100, print_debug=True,\n",
    "             load_first_stage=False, force_pretrain_first_stage=False,\n",
    "             learningRate=0.0001, save_results=True):\n",
    "        \"\"\" trains the model\n",
    "        \n",
    "            X: training data: must be organized as follows:\n",
    "                Number_of_videos, video_length, H, W\n",
    "            epochs: number of epochs to run for final training\n",
    "            pre_epochs: number of epochs for training initial layer\n",
    "            load_first_stage: if True, then the first stage will not\n",
    "                be trained separatly but will be loaded from file\n",
    "                instead\n",
    "        \"\"\"\n",
    "        self.is_trained = True\n",
    "        \n",
    "        X = X.astype('float32')  # hopefully we don't run OOMem here..\n",
    "        \n",
    "        if self.normalize_data:\n",
    "            X -= X.mean(0)[None, :]\n",
    "            X /= X.std(0)[None, :] + X.std() * 0.1\n",
    "\n",
    "        splitter = ImageSplitter(X, n=self.depth+1)\n",
    "\n",
    "        F = self.F\n",
    "        H = self.H\n",
    "        lr = learningRate\n",
    "        dim = splitter.get_dimension()\n",
    "        numpy_rng = np.random.RandomState(1)\n",
    "        \n",
    "        # pretrain the early layer for faster convergence\n",
    "        if load_first_stage:\n",
    "            self.load_first_stage()\n",
    "        \n",
    "        if not self.is_first_layer_trained or force_pretrain_first_stage:\n",
    "            x = tf.placeholder(tf.float32, [None, dim])\n",
    "            y = tf.placeholder(tf.float32, [None, dim])\n",
    "    \n",
    "            if self.is_first_layer_trained:\n",
    "                U1 = tf.Variable(self.U1_np)\n",
    "                V1 = tf.Variable(self.V1_np)\n",
    "                W1 = tf.Variable(self.W1_np)\n",
    "\n",
    "                b_U1 = tf.Variable(self.b_U1_np)\n",
    "                b_V1 = tf.Variable(self.b_V1_np)\n",
    "                b_W1 = tf.Variable(self.b_W1_np)\n",
    "                b_U1_out = tf.Variable(self.b_U1_out_np)\n",
    "                b_V1_out = tf.Variable(self.b_V1_out_np)\n",
    "                b_W1_out = tf.Variable(self.b_W1_out_np)\n",
    "            else:\n",
    "                U1 = tf.Variable(tf.random_normal(shape=(dim, F)) * 0.01)\n",
    "                V1 = tf.Variable(tf.random_normal(shape=(dim, F)) * 0.01)\n",
    "                W1 = tf.Variable(\n",
    "                    numpy_rng.uniform(low=-0.01, high=+0.01, \n",
    "                                      size=( F, H)).astype('float32'))\n",
    "\n",
    "                b_U1 = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "                b_V1 = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "                b_W1 = tf.Variable(np.zeros(H, dtype='float32'))\n",
    "                b_U1_out = tf.Variable(np.zeros(dim, dtype='float32'))\n",
    "                b_V1_out = tf.Variable(np.zeros(dim, dtype='float32'))\n",
    "                b_W1_out = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "\n",
    "            m1 = tf.sigmoid(tf.matmul(tf.multiply(\n",
    "                tf.matmul(x,U1) + b_U1,\n",
    "                tf.matmul(y,V1) + b_V1), W1) + b_W1)\n",
    "\n",
    "            ox = tf.matmul(tf.multiply(\n",
    "                    tf.matmul(m1,tf.transpose(W1)) + b_W1_out,\n",
    "                    tf.matmul(y,V1) + b_V1),tf.transpose(U1))+ b_U1_out\n",
    "            oy = tf.matmul(tf.multiply(\n",
    "                    tf.matmul(m1,tf.transpose(W1)) + b_W1_out,\n",
    "                    tf.matmul(x,U1) + b_U1), \n",
    "                tf.transpose(V1)) + b_V1_out\n",
    "\n",
    "            cost_1 = tf.nn.l2_loss(ox-x) + tf.nn.l2_loss(oy-y)\n",
    "            optimizer_1 = tf.train.AdamOptimizer(learning_rate=lr)\\\n",
    "                .minimize(cost_1)\n",
    "\n",
    "            norm_U1 = tf.nn.l2_normalize(U1, [0,1], epsilon=1e-12, name=None)\n",
    "            norm_V1 = tf.nn.l2_normalize(V1, [0,1], epsilon=1e-12, name=None)\n",
    "\n",
    "            normalize_U1 = U1.assign(norm_U1)\n",
    "            normalize_V1 = V1.assign(norm_V1)\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                init = tf.global_variables_initializer()\n",
    "                sess.run(init)\n",
    "\n",
    "                test_set = splitter.get_test(ngram=2)\n",
    "                X_ = test_set[:,0,:]\n",
    "                Y_ = test_set[:,1,:]\n",
    "                n = test_set.shape[0]\n",
    "                for epoch in range(pre_epochs):\n",
    "                    while splitter.is_same_batch_run():\n",
    "                        batch = splitter.next_batch(ngram=2)\n",
    "                        batch_xs = batch[:,0,:]\n",
    "                        batch_ys = batch[:,1,:]\n",
    "                        sess.run(optimizer_1, feed_dict={x: batch_xs, y: batch_ys})\n",
    "                        sess.run(normalize_U1)\n",
    "                        sess.run(normalize_V1)\n",
    "\n",
    "                    cost_ = sess.run(cost_1, feed_dict={x: X_, y: Y_}) / n\n",
    "                    if print_debug:\n",
    "                        print (\"Pretrain: Epoch: %03d/%03d cost: %.9f\" %\\\n",
    "                                   (epoch,pre_epochs ,cost_) )\n",
    "\n",
    "                self.U1_np = np.array(U1.eval(sess))\n",
    "                self.V1_np = np.array(V1.eval(sess))\n",
    "                self.W1_np = np.array(W1.eval(sess))\n",
    "                self.b_U1_np = np.array(b_U1.eval(sess))\n",
    "                self.b_V1_np = np.array(b_V1.eval(sess))\n",
    "                self.b_W1_np = np.array(b_W1.eval(sess))\n",
    "                self.b_U1_out_np = np.array(b_U1_out.eval(sess))\n",
    "                self.b_V1_out_np = np.array(b_V1_out.eval(sess))\n",
    "                self.b_W1_out_np = np.array(b_W1_out.eval(sess))\n",
    "                self.is_first_layer_trained = True\n",
    "                \n",
    "                if self.modelname is not None and save_results:\n",
    "                    self.save_first_stage()\n",
    "        \n",
    "        if print_debug:\n",
    "            print(\"pre-training ended\")\n",
    "            \n",
    "        # start training the second stage\n",
    "        # set of first stage\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, [None, dim])\n",
    "        y = tf.placeholder(tf.float32, [None, dim])\n",
    "        z = tf.placeholder(tf.float32, [None, dim])\n",
    "        \n",
    "        U1 = tf.Variable(self.U1_np)\n",
    "        V1 = tf.Variable(self.V1_np)\n",
    "        W1 = tf.Variable(self.W1_np)\n",
    "\n",
    "        b_U1 = tf.Variable(self.b_U1_np)\n",
    "        b_V1 = tf.Variable(self.b_V1_np)\n",
    "        b_W1 = tf.Variable(self.b_W1_np)\n",
    "        \n",
    "        m1 = tf.sigmoid(tf.matmul(tf.multiply(\n",
    "                tf.matmul(x,U1) + b_U1,\n",
    "                tf.matmul(y,V1) + b_V1), W1) + b_W1)\n",
    "        \n",
    "        #m2 = tf.sigmoid(tf.matmul(tf.multiply(\n",
    "        #        tf.matmul(y,U1) + b_U1,\n",
    "        #        tf.matmul(z,V1) + b_V1), W1) + b_W1)\n",
    "        \n",
    "        Uy = tf.matmul(y, U1)\n",
    "        WTm1 = tf.matmul(m1, tf.transpose(W1))\n",
    "        \n",
    "        o3 = tf.matmul(tf.multiply(Uy, WTm1), tf.transpose(V1))\n",
    "\n",
    "        cost_2 = tf.nn.l2_loss(o3-z)\n",
    "        optimizer_2 = tf.train.AdamOptimizer(learning_rate=lr)\\\n",
    "            .minimize(cost_2)\n",
    "        \n",
    "        norm_U1 = tf.nn.l2_normalize(U1, [0,1], epsilon=1e-12, name=None)\n",
    "        norm_V1 = tf.nn.l2_normalize(V1, [0,1], epsilon=1e-12, name=None)\n",
    "\n",
    "        normalize_U1 = U1.assign(norm_U1)\n",
    "        normalize_V1 = V1.assign(norm_V1)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "\n",
    "            test_set = splitter.get_test(ngram=3)\n",
    "            X_ = test_set[:,0,:]\n",
    "            Y_ = test_set[:,1,:]\n",
    "            Z_ = test_set[:,2,:]\n",
    "            n = test_set.shape[0]\n",
    "            for epoch in range(epochs):\n",
    "                while splitter.is_same_batch_run():\n",
    "                    batch = splitter.next_batch(ngram=3)\n",
    "                    batch_xs = batch[:,0,:]\n",
    "                    batch_ys = batch[:,1,:]\n",
    "                    batch_zs = batch[:,2,:]\n",
    "                    sess.run(optimizer_2, feed_dict={x: batch_xs, y: batch_ys, z: batch_zs})\n",
    "                    sess.run(normalize_U1)\n",
    "                    sess.run(normalize_V1)\n",
    "\n",
    "                cost_ = sess.run(cost_2, feed_dict={x: X_, y: Y_, z: Z_}) / n\n",
    "                if print_debug:\n",
    "                    print (\"Training: Epoch: %03d/%03d cost: %.9f\" %\\\n",
    "                               (epoch,epochs ,cost_) )\n",
    "\n",
    "        if print_debug:\n",
    "            print(\"training is finished\")\n",
    "            \n",
    "\n",
    "print('xx')\n",
    "\n",
    "model = PredictiveGatingPyramid(depth=2, modelname='test_pgp_300_runs_')\n",
    "model.train(\n",
    "    moving_mnist, \n",
    "    epochs=1000,\n",
    "    pre_epochs=25,\n",
    "    learningRate=0.00000001,\n",
    "    save_results=False,\n",
    "    load_first_stage=True,\n",
    "    force_pretrain_first_stage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.V1_np.shape)\n",
    "print(model.U1_np.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "# function copied from the original code\n",
    "def dispims(M, height, width, border=0, bordercolor=0.0, layout=None, **kwargs):\n",
    "    from pylab import cm, ceil\n",
    "    numimages = M.shape[1]\n",
    "    if layout is None:\n",
    "        n0 = int(np.ceil(np.sqrt(numimages)))\n",
    "        n1 = int(np.ceil(np.sqrt(numimages)))\n",
    "    else:\n",
    "        n0, n1 = layout\n",
    "    im = bordercolor * np.ones(((height+border)*n0+border,(width+border)*n1+border),dtype='<f8')\n",
    "    for i in range(n0):\n",
    "        for j in range(n1):\n",
    "            if i*n1+j < M.shape[1]:\n",
    "                im[i*(height+border)+border:(i+1)*(height+border)+border,\n",
    "                   j*(width+border)+border :(j+1)*(width+border)+border] = np.vstack((\n",
    "                            np.hstack((np.reshape(M[:,i*n1+j],(height, width)),\n",
    "                                   bordercolor*np.ones((height,border),dtype=float))),\n",
    "                            bordercolor*np.ones((border,width+border),dtype=float)\n",
    "                            ))\n",
    "    \n",
    "    return im\n",
    "\n",
    "\n",
    "I = dispims(model.U1_np, 64, 64, 10)\n",
    "J = dispims(model.V1_np, 64, 64, 10)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "fig.add_subplot(121).imshow(I)\n",
    "fig.add_subplot(122).imshow(J)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
