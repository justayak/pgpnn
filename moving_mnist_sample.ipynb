{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "from utils import plot_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 3, 4096)\n"
     ]
    }
   ],
   "source": [
    "class ImageSplitter:\n",
    "    \"\"\" splits the moving mnist dataset into shorts of n frames\n",
    "    \n",
    "    Moving Mnist: \n",
    "        10.000 sequences of length 20 showing 2 digits moving in 64x64\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n=3, \n",
    "                 ntest=1000,\n",
    "                 file_name = 'mnist_test_seq.npy',\n",
    "                 url = 'http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy',\n",
    "                 batchsize=32):\n",
    "        \"\"\" ctor\n",
    "            n: integer, length of subsequence\n",
    "            ntest: integer, number of items that belong to the test set\n",
    "            file_name: string, name used to safe the data locally\n",
    "            url: url from where data can be downloaded\n",
    "            batchsize: ~\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.batchsize = batchsize\n",
    "        self.batch_loop = 0\n",
    "        self.n = n\n",
    "        \n",
    "        if not os.path.isfile(file_name):\n",
    "            print(\"could not find dataset: download it..\")\n",
    "            urllib.request.urlretrieve(url, file_name)\n",
    "            print(\"download complete\")\n",
    "    \n",
    "        moving_mnist = np.load(file_name) # shape: 20,10000,64,64\n",
    "        \n",
    "        moving_mnist = np.rollaxis(moving_mnist, 1) # --> 10000,20,64,64\n",
    "        np.random.shuffle(moving_mnist)\n",
    "        \n",
    "        self.test_set = moving_mnist[0:ntest]\n",
    "        self.train_set = moving_mnist[ntest:]\n",
    "        \n",
    "        F, N, H, W = moving_mnist.shape # Frames, Numbers, Height, Width\n",
    "        \n",
    "        \n",
    "        self.vector_dimension = H * W\n",
    "        \n",
    "    def get_dimension(self):\n",
    "        \"\"\" gets the data dimension\n",
    "        \"\"\"\n",
    "        return self.vector_dimension\n",
    "\n",
    "    def transform_to_n_gram(self, batch):\n",
    "        \"\"\" transforms the batch into an n-gram (parameter n)\n",
    "        batch: np.array((batchsize, 20, 64, 64))\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        N, M, H, W = batch.shape\n",
    "        seqs = M-n+1 # sequences per video\n",
    "        \n",
    "        Result = np.zeros((seqs * N, n, 64, 64), 'uint8')\n",
    "        \n",
    "        pos = 0\n",
    "        for j in range(N):\n",
    "            for i in range(seqs):\n",
    "                Result[pos] = batch[j, i:i+n]\n",
    "                pos += 1 \n",
    "        \n",
    "        N, M, H, W = Result.shape\n",
    "        return Result.reshape((N, M, H * W))\n",
    "        \n",
    "    \n",
    "    def next_batch(self):\n",
    "        \"\"\" returns the next batch\n",
    "        \"\"\"\n",
    "        start = self.batch_loop\n",
    "        end = self.batch_loop + self.batchsize\n",
    "        N = self.train_set.shape[0]\n",
    "        if N > end:\n",
    "            self.batch_loop = end\n",
    "            return self.transform_to_n_gram(\n",
    "                self.train_set[start:end])\n",
    "        else:\n",
    "            diff = (N - start)\n",
    "            end = self.batchsize - diff\n",
    "            \n",
    "            set1 = self.train_set[start:N]\n",
    "            set2 = self.train_set[0:end]\n",
    "            \n",
    "            self.batch_loop = end\n",
    "            return self.transform_to_n_gram(\n",
    "                np.concatenate((set1, set2)))\n",
    "        \n",
    "\n",
    "splitter = ImageSplitter()\n",
    "\n",
    "out = splitter.next_batch()\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Util\n",
    "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n",
    "    \"\"\"Returns a tensor with random binomial distribution of values.\n",
    "    # Arguments\n",
    "        shape: A tuple of integers, the shape of tensor to create.\n",
    "        p: A float, `0. <= p <= 1`, probability of binomial distribution.\n",
    "        dtype: String, dtype of returned tensor.\n",
    "        seed: Integer, random seed.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    if dtype is None:\n",
    "        dtype = 'float32'\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(10e6)\n",
    "    return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n",
    "                    tf.ones(shape, dtype=dtype),\n",
    "                    tf.zeros(shape, dtype=dtype))\n",
    "\n",
    "\n",
    "\n",
    "numpy_rng = np.random.RandomState(1)\n",
    "\n",
    "dim = splitter.get_dimension()\n",
    "numfac  = 512\n",
    "nummap  = 256\n",
    "\n",
    "# pretrainig first layer\n",
    "input_x1 = tf.placeholder(tf.float32, [None, dim])\n",
    "input_x2 = tf.placeholder(tf.float32, [None, dim])\n",
    "\n",
    "U1 = tf.Variable(tf.random_normal(shape=(dim, numfac)) * 0.01)\n",
    "V1 = tf.Variable(tf.random_normal(shape=(dim, numfac)) * 0.01)\n",
    "W1 = tf.Variable(numpy_rng.uniform(low=-0.01, high=+0.01, size=( numfac, nummap)).astype('float32'))\n",
    "\n",
    "bias_U1 = tf.Variable(np.zeros(numfac, dtype='float32'))\n",
    "bias_V1 = tf.Variable(np.zeros(numfac, dtype='float32'))\n",
    "bias_W1 = tf.Variable(np.zeros(nummap, dtype='float32'))\n",
    "bias_U1_out = tf.Variable(np.zeros(dim, dtype='float32'))\n",
    "bias_V1_out = tf.Variable(np.zeros(dim, dtype='float32'))\n",
    "bias_W1_out = tf.Variable(np.zeros(numfac, dtype='float32'))\n",
    "\n",
    "# m = sigmoid(W(U*x_1 . V*x_2 ))\n",
    "M1 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(input_x1,U1) + bias_U1,tf.matmul(input_x2,V1) + bias_V1), W1)+ bias_W1)\n",
    "\n",
    "output_x1 = tf.matmul(tf.multiply(tf.matmul(M1,tf.transpose(W1)) + bias_W1_out,tf.matmul(input_x2,V1) + bias_V1),tf.transpose(U1))+ bias_U1_out\n",
    "output_x2 = tf.matmul(tf.multiply(tf.matmul(M1,tf.transpose(W1)) + bias_W1_out,tf.matmul(input_x1,U1) + bias_U1), tf.transpose(V1))+ bias_V1_out\n",
    "\n",
    "cost_1 = tf.nn.l2_loss(output_x1-input_x1) + tf.nn.l2_loss(output_x2-input_x2)\n",
    "optimizer_1 = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost_1)\n",
    "\n",
    "print('xx')\n",
    "\n",
    "#class PredictiveGatingPyramid:\n",
    "#    \"\"\"\n",
    "#    \"\"\"\n",
    "#    \n",
    "#    def __init__(self):\n",
    "#        pass\n",
    "#pgp = PredictiveGatingPyramid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
