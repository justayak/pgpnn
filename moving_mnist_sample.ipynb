{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "from utils import plot_mats\n",
    "\n",
    "file_name = 'mnist_test_seq.npy'\n",
    "url = 'http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy'\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    print(\"could not find dataset: download it..\")\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(\"download complete\")\n",
    "\n",
    "# Moving Mnist: 10.000 sequences of length 20 showing 2 digits moving in 64x64\n",
    "moving_mnist = np.load(file_name) # shape: 20,10000,64,64\n",
    "moving_mnist = np.rollaxis(moving_mnist, 1) # --> 10000,20,64,64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ImageSplitter:\n",
    "    \"\"\" splits the video set into shorts of n frames\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 dataset,\n",
    "                 n=3, \n",
    "                 ntest=256,\n",
    "                 batchsize=64):\n",
    "        \"\"\" ctor\n",
    "            dataset: list of videos that have to be cut into n-grams\n",
    "            n: integer, length of subsequence\n",
    "            ntest: integer, number of items that belong to the test set\n",
    "            batchsize: ~\n",
    "            \n",
    "        The data has to be arranged in following order:\n",
    "            NUMBER_OF_VIDEOS, LENGTH_OF_VIDEO, HEIGHT, WIDTH\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.batchsize = batchsize\n",
    "        self.batch_loop = 0\n",
    "        self.n = n\n",
    "        \n",
    "\n",
    "        np.random.shuffle(dataset)\n",
    "        \n",
    "        self.test_set = dataset[0:ntest]\n",
    "        self.train_set = dataset[ntest:]\n",
    "        \n",
    "        F, N, H, W = dataset.shape # Frames, Numbers, Height, Width\n",
    "        \n",
    "        \n",
    "        self.vector_dimension = H * W\n",
    "        self.same_batch_run = True\n",
    "        \n",
    "    def get_dimension(self):\n",
    "        \"\"\" gets the data dimension\n",
    "        \"\"\"\n",
    "        return self.vector_dimension\n",
    "\n",
    "    def transform_to_n_gram(self, batch, ngram=None):\n",
    "        \"\"\" transforms the batch into an n-gram (parameter n)\n",
    "        batch: np.array((batchsize, 20, 64, 64))\n",
    "        \"\"\"\n",
    "        if ngram is None:\n",
    "            n = self.n\n",
    "        else:\n",
    "            n = ngram\n",
    "        N, M, H, W = batch.shape\n",
    "        seqs = M-n+1 # sequences per video\n",
    "        \n",
    "        Result = np.zeros((seqs * N, n, H, W))\n",
    "        \n",
    "        pos = 0\n",
    "        for j in range(N):\n",
    "            for i in range(seqs):\n",
    "                Result[pos] = batch[j, i:i+n]\n",
    "                pos += 1\n",
    "        \n",
    "        N, M, H, W = Result.shape\n",
    "        return Result.reshape((N, M, H * W))\n",
    "    \n",
    "    def is_same_batch_run(self):\n",
    "        if self.same_batch_run:\n",
    "            return True\n",
    "        else:\n",
    "            self.same_batch_run = True\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def next_batch(self, ngram=None):\n",
    "        \"\"\" returns the next batch\n",
    "        \"\"\"\n",
    "        start = self.batch_loop\n",
    "        end = self.batch_loop + self.batchsize\n",
    "        N = self.train_set.shape[0]\n",
    "        if N > end:\n",
    "            self.same_batch_run = True\n",
    "            self.batch_loop = end\n",
    "            return self.transform_to_n_gram(\n",
    "                self.train_set[start:end], ngram=ngram)\n",
    "        else:\n",
    "            self.same_batch_run = False\n",
    "            diff = (N - start)\n",
    "            end = self.batchsize - diff\n",
    "            \n",
    "            set1 = self.train_set[start:N]\n",
    "            set2 = self.train_set[0:end]\n",
    "            \n",
    "            self.batch_loop = end\n",
    "            return self.transform_to_n_gram(\n",
    "                np.concatenate((set1, set2)), ngram=ngram)\n",
    "    \n",
    "    def get_test(self, ngram=None):\n",
    "        \"\"\" returns the test data as ngram\n",
    "        \"\"\"\n",
    "        return self.transform_to_n_gram(self.test_set, ngram=ngram)\n",
    "\n",
    "# splitter = ImageSplitter(moving_mnist, n=3)\n",
    "# batch = splitter.next_batch(ngram=2)\n",
    "# print(batch[:,0,:].shape)\n",
    "\n",
    "# test = splitter.get_test(ngram=2)\n",
    "# print(test.shape)\n",
    "\n",
    "# print(np.max(test), np.min(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx\n",
      "Pretrain: Epoch: 000/100 cost: 2155.036184211\n",
      "Pretrain: Epoch: 001/100 cost: 2017.787006579\n",
      "Pretrain: Epoch: 002/100 cost: 1953.709292763\n",
      "Pretrain: Epoch: 003/100 cost: 1919.534539474\n",
      "Pretrain: Epoch: 004/100 cost: 1896.626233553\n",
      "Pretrain: Epoch: 005/100 cost: 1880.099300987\n",
      "Pretrain: Epoch: 006/100 cost: 1866.913445724\n",
      "Pretrain: Epoch: 007/100 cost: 1855.603618421\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# this function is 'borrowed' from keras\n",
    "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n",
    "    \"\"\"Returns a tensor with random binomial distribution of values.\n",
    "    # Arguments\n",
    "        shape: A tuple of integers, the shape of tensor to create.\n",
    "        p: A float, `0. <= p <= 1`, probability of binomial distribution.\n",
    "        dtype: String, dtype of returned tensor.\n",
    "        seed: Integer, random seed.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    if dtype is None:\n",
    "        dtype = 'float32'\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(10e6)\n",
    "    return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n",
    "                    tf.ones(shape, dtype=dtype),\n",
    "                    tf.zeros(shape, dtype=dtype))\n",
    "\n",
    "\n",
    "class PredictiveGatingPyramid:\n",
    "    \"\"\" implementation of the pgp\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 depth=2, \n",
    "                 numfilters=512, \n",
    "                 numHidden=256,\n",
    "                 modelname=None,\n",
    "                 learning_rate=0.01,\n",
    "                 normalize_data=True):\n",
    "        assert depth > 0\n",
    "        assert depth == 2, \"Other depth than 2 not supported\"\n",
    "        self.depth = depth\n",
    "        self.F = numfilters\n",
    "        self.H = numHidden\n",
    "        self.learningRate = learning_rate\n",
    "        self.is_trained = False\n",
    "        self.normalize_data = normalize_data\n",
    "        self.modelname = modelname\n",
    "        self.is_first_layer_trained = False\n",
    "        \n",
    "        \n",
    "    def save_first_stage(self):\n",
    "        \"\"\" saves the model onto disk\n",
    "        \"\"\"\n",
    "        modelname = self.modelname\n",
    "        assert(self.is_trained)\n",
    "        assert modelname is not None\n",
    "        np.save(modelname + \"U1\", self.U1_np)\n",
    "        np.save(modelname + \"V1\", self.V1_np)\n",
    "        np.save(modelname + \"W1\", self.W1_np)\n",
    "        np.save(modelname + \"b_U1\", self.b_U1_np)\n",
    "        np.save(modelname + \"b_V1\", self.b_V1_np)\n",
    "        np.save(modelname + \"b_W1\", self.b_W1_np)\n",
    "        np.save(modelname + \"b_U1_out\", self.b_U1_out_np)\n",
    "        np.save(modelname + \"b_V1_out\", self.b_V1_out_np)\n",
    "        np.save(modelname + \"b_W1_out\", self.b_W1_out_np)\n",
    "    \n",
    "    def load_first_stage(self, modelname=None):\n",
    "        \"\"\" loads the first layer of the network\n",
    "        \"\"\"\n",
    "        if modelname is None:\n",
    "            modelname = self.modelname\n",
    "        self.U1_np = np.load(modelname + \"U1.npy\")\n",
    "        self.V1_np = np.load(modelname + \"V1.npy\")\n",
    "        self.W1_np = np.load(modelname + \"W1.npy\")\n",
    "        self.b_U1_np = np.load(modelname + \"b_U1.npy\")\n",
    "        self.b_V1_np = np.load(modelname + \"b_V1.npy\")\n",
    "        self.b_W1_np = np.load(modelname + \"b_W1.npy\")\n",
    "        self.b_U1_out_np = np.load(modelname + \"b_U1_out.npy\")\n",
    "        self.b_V1_out_np = np.load(modelname + \"b_V1_out.npy\")\n",
    "        self.b_W1_out_np = np.load(modelname + \"b_W1_out.npy\")\n",
    "        self.is_first_layer_trained = True\n",
    "    \n",
    "    \n",
    "    def train(self, X, epochs=100, pre_epochs=100, print_debug=True,\n",
    "             load_first_stage=False):\n",
    "        \"\"\" trains the model\n",
    "        \n",
    "            X: training data: must be organized as follows:\n",
    "                Number_of_videos, video_length, H, W\n",
    "            epochs: number of epochs to run for final training\n",
    "            pre_epochs: number of epochs for training initial layer\n",
    "            load_first_stage: if True, then the first stage will not\n",
    "                be trained separatly but will be loaded from file\n",
    "                instead\n",
    "        \"\"\"\n",
    "        self.is_trained = True\n",
    "        \n",
    "        X = X.astype('float32')  # hopefully we don't run OOMem here..\n",
    "        \n",
    "        if self.normalize_data:\n",
    "            X -= X.mean(0)[None, :]\n",
    "            X /= X.std(0)[None, :] + X.std() * 0.1\n",
    "\n",
    "        splitter = ImageSplitter(X, n=self.depth+1)\n",
    "\n",
    "        F = self.F\n",
    "        H = self.H\n",
    "        lr = self.learningRate\n",
    "        dim = splitter.get_dimension()\n",
    "        numpy_rng = np.random.RandomState(1)\n",
    "        \n",
    "        # pretrain the early layer for faster convergence\n",
    "        if load_first_stage:\n",
    "            self.load_first_stage()\n",
    "        \n",
    "        if not self.is_first_layer_trained:\n",
    "            x = tf.placeholder(tf.float32, [None, dim])\n",
    "            y = tf.placeholder(tf.float32, [None, dim])\n",
    "\n",
    "            U1 = tf.Variable(tf.random_normal(shape=(dim, F)) * 0.01)\n",
    "            V1 = tf.Variable(tf.random_normal(shape=(dim, F)) * 0.01)\n",
    "            W1 = tf.Variable(\n",
    "                numpy_rng.uniform(low=-0.01, high=+0.01, \n",
    "                                  size=( F, H)).astype('float32'))\n",
    "\n",
    "            b_U1 = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "            b_V1 = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "            b_W1 = tf.Variable(np.zeros(H, dtype='float32'))\n",
    "            b_U1_out = tf.Variable(np.zeros(dim, dtype='float32'))\n",
    "            b_V1_out = tf.Variable(np.zeros(dim, dtype='float32'))\n",
    "            b_W1_out = tf.Variable(np.zeros(F, dtype='float32'))\n",
    "\n",
    "            m1 = tf.sigmoid(tf.matmul(tf.multiply(\n",
    "                tf.matmul(x,U1) + b_U1,\n",
    "                tf.matmul(y,V1) + b_V1), W1) + b_W1)\n",
    "\n",
    "            ox = tf.matmul(tf.multiply(\n",
    "                    tf.matmul(m1,tf.transpose(W1)) + b_W1_out,\n",
    "                    tf.matmul(y,V1) + b_V1),tf.transpose(U1))+ b_U1_out\n",
    "            oy = tf.matmul(tf.multiply(\n",
    "                    tf.matmul(m1,tf.transpose(W1)) + b_W1_out,\n",
    "                    tf.matmul(x,U1) + b_U1), \n",
    "                tf.transpose(V1)) + b_V1_out\n",
    "\n",
    "            cost_1 = tf.nn.l2_loss(ox-x) + tf.nn.l2_loss(oy-y)\n",
    "            optimizer_1 = tf.train.AdamOptimizer(learning_rate=0.0001)\\\n",
    "                .minimize(cost_1)\n",
    "\n",
    "            norm_U1 = tf.nn.l2_normalize(U1, [0,1], epsilon=1e-12, name=None)\n",
    "            norm_V1 = tf.nn.l2_normalize(V1, [0,1], epsilon=1e-12, name=None)\n",
    "\n",
    "            normalize_U1 = U1.assign(norm_U1)\n",
    "            normalize_V1 = V1.assign(norm_V1)\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                init = tf.global_variables_initializer()\n",
    "                sess.run(init)\n",
    "\n",
    "                test_set = splitter.get_test(ngram=2)\n",
    "                X_ = test_set[:,0,:]\n",
    "                Y_ = test_set[:,1,:]\n",
    "                n = test_set.shape[0]\n",
    "                for epoch in range(pre_epochs):\n",
    "                    while splitter.is_same_batch_run():\n",
    "                        batch = splitter.next_batch(ngram=2)\n",
    "                        batch_xs = batch[:,0,:]\n",
    "                        batch_ys = batch[:,1,:]\n",
    "                        sess.run(optimizer_1, feed_dict={x: batch_xs, y: batch_ys})\n",
    "                        sess.run(normalize_U1)\n",
    "                        sess.run(normalize_V1)\n",
    "\n",
    "                    cost_ = sess.run(cost_1, feed_dict={x: X_, y: Y_}) / n\n",
    "                    if print_debug:\n",
    "                        print (\"Pretrain: Epoch: %03d/%03d cost: %.9f\" %\\\n",
    "                                   (epoch,pre_epochs ,cost_) )\n",
    "\n",
    "                self.U1_np = np.array(U1.eval(sess))\n",
    "                self.V1_np = np.array(V1.eval(sess))\n",
    "                self.W1_np = np.array(W1.eval(sess))\n",
    "                self.b_U1_np = np.array(b_U1.eval(sess))\n",
    "                self.b_V1_np = np.array(b_V1.eval(sess))\n",
    "                self.b_W1_np = np.array(b_W1.eval(sess))\n",
    "                self.b_U1_out_np = np.array(b_U1_out.eval(sess))\n",
    "                self.b_V1_out_np = np.array(b_V1_out.eval(sess))\n",
    "                self.b_W1_out_np = np.array(b_W1_out.eval(sess))\n",
    "                self.is_first_layer_trained = True\n",
    "                \n",
    "                if self.modelname is not None:\n",
    "                    self.save_first_stage()\n",
    "        \n",
    "        print(\"train end\")\n",
    "\n",
    "print('xx')\n",
    "\n",
    "model = PredictiveGatingPyramid(depth=2, modelname='test_pgp')\n",
    "model.train(moving_mnist, pre_epochs=100, load_first_stage=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
